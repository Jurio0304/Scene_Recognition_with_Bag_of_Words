import cv2
import numpy as np
from skimage.io import imread_collection
from skimage.feature import hog
from sklearn.cluster import KMeans, MiniBatchKMeans
import time


def build_vocabulary_sift(image_paths, vocab_size):
    """
    This function should sample SIFT descriptors from the training images,
    cluster them with kmeans, and then return the cluster centers.

    """

    images = imread_collection(image_paths)

    images_feature_vectors = []

    for image in images:
        sift = cv2.SIFT_create()
        keypoints, descriptors = sift.detectAndCompute(image, None)

        images_feature_vectors.append(descriptors)

    images_feature_vectors = np.vstack(images_feature_vectors)

    # MiniBatchKMeans相比KMeans用时更少，资源消耗更少，质量相对要差一点，但差别不大
    t0 = time.time()
    # k_means = KMeans(n_clusters=vocab_size, max_iter=500).fit(images_feature_vectors)
    k_means = MiniBatchKMeans(n_clusters=vocab_size, max_iter=500).fit(images_feature_vectors)
    print('time spend：', time.time() - t0)

    vocabulary = np.vstack(k_means.cluster_centers_)

    return vocabulary


def build_vocabulary(image_paths, vocab_size):
    """
    This function should sample HOG descriptors from the training images,
    cluster them with kmeans, and then return the cluster centers.

    Inputs:
        image_paths: a Python list of image path strings
         vocab_size: an integer indicating the number of words desired for the
                     bag of words vocab set

    Outputs:
        a vocab_size x (z*z*9) (see below) array which contains the cluster
        centers that result from the K Means clustering.

    You'll need to generate HOG features using the skimage.feature.hog() function.
    The documentation is available here:
    http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog

    However, the documentation is a bit confusing, so we will highlight some
    important arguments to consider:
        cells_per_block: The hog function breaks the image into evenly-sized
            blocks, which are further broken down into cells, each made of
            pixels_per_cell pixels (see below). Setting this parameter tells the
            function how many cells to include in each block. This is a tuple of
            width and height. Your SIFT implementation, which had a total of
            16 cells, was equivalent to setting this argument to (4,4).
        pixels_per_cell: This controls the width and height of each cell
            (in pixels). Like cells_per_block, it is a tuple. In your SIFT
            implementation, each cell was 4 pixels by 4 pixels, so (4,4).
        feature_vector: This argument is a boolean which tells the function
            what shape it should use for the return array. When set to True,
            it returns one long array. We recommend setting it to True and
            reshaping the result rather than working with the default value,
            as it is very confusing.

    It is up to you to choose your cells per block and pixels per cell. Choose
    values that generate reasonably-sized feature vectors and produce good
    classification results. For each cell, HOG produces a histogram (feature
    vector) of length 9. We want one feature vector per block. To do this we
    can append the histograms for each cell together. Let's say you set
    cells_per_block = (z,z). This means that the length of your feature vector
    for the block will be z*z*9.

    With feature_vector=True, hog() will return one long np array containing every
    cell histogram concatenated end to end. We want to break this up into a
    list of (z*z*9) block feature vectors. We can do this using a really nifty numpy
    function. When using np.reshape, you can set the length of one dimension to
    -1, which tells numpy to make this dimension as big as it needs to be to
    accomodate to reshape all of the data based on the other dimensions. So if
    we want to break our long np array (long_boi) into rows of z*z*9 feature
    vectors we can use small_bois = long_boi.reshape(-1, z*z*9).

    The number of feature vectors that come from this reshape is dependent on
    the size of the image you give to hog(). It will fit as many blocks as it
    can on the image. You can choose to resize (or crop) each image to a consistent size
    (therefore creating the same number of feature vectors per image), or you
    can find feature vectors in the original sized image.

    ONE MORE THING
    If we returned all the features we found as our vocabulary, we would have an
    absolutely massive vocabulary. That would make matching inefficient AND
    inaccurate! So we use K Means clustering to find a much smaller (vocab_size)
    number of representative points. We recommend using sklearn.cluster.KMeans
    to do this. Note that this can take a VERY LONG TIME to complete (upwards
    of ten minutes for large numbers of features and large max_iter), so set
    the max_iter argument to something low (we used 100) and be patient. You
    may also find success setting the "tol" argument (see documentation for
    details)
    """

    images = imread_collection(image_paths)

    cells_per_block = (2, 2)
    pixels_per_cell = (4, 4)
    t = cells_per_block[0]
    images_feature_vectors = []

    for image in images:
        feature_vector = hog(
            image,
            feature_vector=True,
            pixels_per_cell=pixels_per_cell,
            cells_per_block=cells_per_block,
            visualize=False
        ).reshape(-1, t * t * 9)

        images_feature_vectors.append(feature_vector)

    images_feature_vectors = np.vstack(images_feature_vectors)

    # MiniBatchKMeans相比KMeans用时更少，资源消耗更少，质量相对要差一点，但差别不大
    t0 = time.time()
    # k_means = KMeans(n_clusters=vocab_size, max_iter=500).fit(images_feature_vectors)
    k_means = MiniBatchKMeans(n_clusters=vocab_size, max_iter=500).fit(images_feature_vectors)
    print('time spend：', time.time() - t0)

    vocabulary = np.vstack(k_means.cluster_centers_)

    return vocabulary
